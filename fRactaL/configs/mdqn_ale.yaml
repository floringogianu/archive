# DQN hyperparams following Rainbow. Follows hyperparams from Dopamine:
# github.com/google/dopamine/blob/master/dopamine/agents/dqn/configs/dqn.gin

game: Pong

experiment: M-DQN
device: cuda
save: yes
replay_save_freq: 2

epoch_cnt: 200
train_step_cnt: 50000
valid_step_cnt: 50000
val_epsilon: 0.001  # validation epsilon greedy

agent:
  name: M-DQN
  args_:
    epsilon: [linear, 1.0, 0.01, 250000, 20000]
    gamma: 0.99
    loss_fn: MSELoss
    update_freq: 4
    target_update_freq: 8000

replay_:
  capacity: 1000000
  batch_size: 32
  hist_len: 4
  device: cuda

estimator:
  name: AtariNet
  args_:
    hidden_size: 512
    hist_len: 4
    spectral: null  # Usage: -1 or -2 or combined -1,-2,-3.
    lipschitz_k: 5
    random_power_iteration: yes
    initializer: "xavier_uniform"  # this field can be missing

pretrained:
  source: "results/2022Jun20-184919_F256_3v2_2lr_slide/0001_model.args_.fc_layers__512__512__512_/0/model_0499.pkl"
  layers: [0,3] # range
  freeze: no

optim:
  name: "Adam"
  args_:
    lr: 0.0000625
    eps: 0.0001500
